# Music Streaming Data Warehouse with Amazon Web Service
## Introduction

A startup called Sparkify wants to analyze the data they've been collecting on
songs and user activity on their new music streaming app. The analytics team is
particularly interested in understanding what songs users are listening to.
Currently, they don't have an easy way to query their data, which resides in a
directory of JSON logs on user activity on the app, as well as a directory with
JSON metadata on the songs in their app.

As their data engineer, I am tasked with building an ETL pipeline that extracts
their data from S3, stages them in Redshift, and transforms data into a set of
dimensional tables for their analytics team to continue finding insights into
what songs their users are listening to.

## Music data format

In the project, I work with two datasets that reside in S3. Here are the S3
links for each:

* Song data: `s3://udacity-dend/song_data`
* Log data: `s3://udacity-dend/log_data`

Log data json path: `s3://udacity-dend/log_json_path.json`

### Song dataset
Each file is in JSON format and contains metadata about a song and the artist
of that song. The files are partitioned by the first three letters of each
song's track ID. For example, here are file paths to two files in this dataset.

```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```

And below is an example of what a single song file, `TRAABJL12903CDCF1A.json`,
looks like.
```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

### Log dataset
The second dataset consists of log files in JSON format generated by an event
simulator based on the songs in the dataset above. These simulate app activity
logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset are partitioned by year and  month. For example,
here are file paths to two files in this dataset.

```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```

And below is an example of what the data in a log file, `2018-11-12-events.json`,
looks like.

<img src="./images/Log data.png" width="1000" height="300">

## Redshift database design

My Redshift database has 2 schemas:

* `staging`: contains staging tables that are used to store intermediate data
extracted from S3 bucket.
* `datamart`: contains data mart tables that supports songplay analysis.

### Staging

There are 2 staging tables: `staging_events` for the log data and `staging_songs`
for the song data. The  figure illustrates the structure of 2 staging tables:

<img src="./images/Staging ERD.png" width="400" height="500">

### Data mart

The Star Data Mart Schema is implemented in this ETL pipeline. Concretely, there
is 1 fact table `songplays` that keeps users' music listening events. Surrounding
the fact table are 4 dimension tables `users`, `artists`, `songs` and `time`,
which store entity details for further retrieval by means of `JOIN` operator. The
figure below best describes the model's architecture:

<img src="./images/Data Mart ERD.png" width="800" height="600">

With the above architecture, you can easily query basic information of an event
directly from `songplays`. Moreover, you can get the event's details, such as
user information or listened song information, simply by joining the fact table
with the corresponding dimension table.

About the distribution of data on the Redshift cluster, 3 dimension tables
`artists`, `songs` and `time` are distributed in the `ALL` style since they
contain so few data that can be completely stored on each node. Dimension table
`users` and fact table `songplays` are stored using the distribution key
`user_id` because they are huge and should be distributed under the same key - 
`user_id` for more performant `JOIN`.

## Directory structure

The overall directory structure of the project is as follow:
```
project_root
│   create_redshift.ipynb
│   create_tables.py
│   dwh.cfg
│   etl.py
│   README.md
│   sql_queries.py
│   test.ipynb
│
└───images
    │   $visual_name.png
    │   ...
```

Where:

1. `create_redshift.ipynb` provides an environment to create IAM role and
Redshift cluster.
2. `create_tables.py` contains the script to reset the database (drop and
create tables)
3. `dwh.cfg` contains the configs of the AWS services that are used.
4. `etl.py` contains the script to perform the ETL pipeline.
5. `README.md` is a markdown document of this project.
6. `sql_queries.py` contains all the SQL queries for data modeling and ETL
process.
7. `test.ipynb` provides an environment to explore the data and test everything
at the end once the ETL pipeline is completed.
8. `images` contains necessary images for documentation and visualization.

## Instructions
### Prerequisites

These are the prerequisites to run the codes:
* Python 3.7
* PostgreSQL
* psycopg2 Python library
* Jupyter Notebook
* Amazon Service account with a pre-registered user

### How to run

1. Customize the configuration of the AWS services at `dwh.cfg`.
2. Create necessary AWS services using `create_redshift.ipynb` notebook.
3. Navigate your present working directory to the project's directory.
4. Run `create_tables.py` to create/reset the database by:
   ```
   python create_tables.py
   ```
5. Run `etl.py` to perform the ETL pipeline process by:
   ```
   python etl.py
   ```
6. Now you've got a complete Music Streaming data warehouse residing on Amazon
Redshift cluster. You can query and visualize the data, or check for the
database constraints at `test.ipynb`.

## Example queries

Now let's perform some basic analytics on the implemented
database.

### Query 1

Supposed you want to get the top 5 locations where the Streaming
system is used the most. This can be accomplished simply by
ordering the number of rows in the fact table `songplays` grouped
by `location` as follow:
```
SELECT location, COUNT(*)
FROM songplays GROUP BY location
ORDER BY COUNT(*) DESC LIMIT 5
```
And the result is:

<img src="./images/Query 1.png" width="400" height="250">

### Query 2

Now, how about the top 5 interactive users of the Streaming system?
Specifically, the interactivity of a user is determined by how
many times he visits the site and listens to something in that
session. The resulted table should include both the user
information and the number of interactions. This can be answered
using the following query:
```
SELECT user_id, first_name, last_name, gender, level, COUNT(*)
FROM songplays NATURAL JOIN users
GROUP BY user_id, first_name, last_name, gender, level
ORDER BY COUNT(*) DESC LIMIT 5
```
Having executed the above query, the result should be something
like this:

<img src="./images/Query 2.png" width="500" height="250">

## License
Distributed under the MIT License. [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)