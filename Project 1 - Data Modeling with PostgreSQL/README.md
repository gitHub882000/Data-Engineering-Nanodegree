# Music Streaming Data Modeling with PostgreSQL
## Introduction

A startup called Sparkify wants to analyze the data they've been collecting on
songs and user activity on their new music streaming app. The analytics team is
particularly interested in understanding what songs users are listening to.
Currently, they don't have an easy way to query their data, which resides in a
directory of JSON logs on user activity on the app, as well as a directory with
JSON metadata on the songs in their app.

The project is about creating a PostgreSQL database with tables designed to
optimize queries on song play analysis. My role is to create a database schema
and ETL pipeline for this analysis.

## Database schema design

The Star Data Mart Schema is implemented in this ETL pipeline. Concretely, there
is 1 fact table `Songplays` that keeps users' music listening events. Surrounding
the fact table are 4 dimension tables `Users`, `Artists`, `Songs` and `Time`,
which store entity details for further retrieval by means of `JOIN` operator. The
figure below best describes the model's architecture:

<img src="./images/Music Streaming ERD.png" width="800" height="600">

With the above architecture, you can easily query basic information of an event
directly from `Songplays`. Moreover, you can get the event's details, such as
user information or listened song information, simply by joining the fact table
with the corresponding dimension table.

## Directory structure

The overall directory structure of the project is as follow:
```
project_root
│   create_tables.py
│   etl.ipynb
│   etl.py
│   README.md
│   sql_queries.py
│   test.ipynb
│
└───data
│   └───log_data
│   │   └───$year_of_log
│   │   │   └───$month_of_log
│   │   │   │   │   $log_data.json
│   │   │   │   │   ...
│   │   │   │
│   │   │   └───...
│   │   │
│   │   └───...
│   │
│   └───song_data
│       └───$1st_letter_trackid
│       │   └───$2nd_letter_trackid
│       │   │   └───$3rd_letter_trackid
│       │   │   │   │   $song_data.json
│       │   │   │   │   ...
│       │   │   │
│       │   │   └───...
│       │   │
│       │   └───...
│       │
│       └───...
│
└───images
    │   $visual_name.png
    │   ...
```

Where:

1. `create_tables.py` contains the script to reset the database (drop and
create tables)
2. `etl.ipynb` provides an environment to implement ETL functions and visualize
the process more efficiently.
3. `etl.py` contains the script to perform the ETL pipeline. Specifically, it is
a wrapper where multiple functions implemented in `etl.ipynb` are combined to
form a complete ETL pipeline.
4. `README.md` is a markdown document of this project.
5. `sql_queries.py` contains all the SQL queries for data modeling and ETL
process.
6. `test.ipynb` provides an environment to explore the data and test everything
at the end once the ETL pipeline is completed.
7. `data` contains `song_data` and `log_data`, which are described in the
following.
   1. `song_data` contains a subset of real data from the Million Song Dataset.
   Each file is in JSON format and contains metadata about a song and the artist
   of that song. The files are partitioned by the first three letters of each
   song's track ID. And a song data JSON file looks like this.
      ```
      {"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
      ```
   2. `log_data` consists of log files in JSON format generated by this event
   simulator based on the songs in the dataset above. These simulate activity
   logs from a music streaming app based on specified configurations. The log
   files in the dataset are partitioned by year and month. And below is an
   example of what the data in a log file looks like.
   <img src="./images/Log data.png" width="1000" height="300">
8. `images` contains necessary images for documentation and visualization.

## Instructions
### Prerequisites

These are the prerequisites to run the codes:
* Python 3.7
* PostgreSQL
* psycopg2 Python library
* Jupyter Notebook

### How to run

1. Add your own song and log datasets with the pre-defined JSON format to the
`data` folder.
2. Navigate your present working directory to the project's directory.
3. Run `create_tables.py` to create/reset the database by:
   ```
   python create_tables.py
   ```
4. Run `etl.py` to perform the ETL pipeline process by:
   ```
   python etl.py
   ```
5. Now you've got a complete Music Streaming PostgreSQL database. You can query
and visualize the data, or check for the database constraints at `test.ipynb`.

## Example queries

Now let's perform some basic analytics on the implemented
database.

### Query 1

Supposed you want to get the top 5 locations where the Streaming
system is used the most. This can be accomplished simply by
ordering the number of rows in the fact table `songplays` grouped
by `location` as follow:
```
SELECT location, COUNT(*)
FROM songplays GROUP BY location
ORDER BY COUNT(*) DESC LIMIT 5
```
And the result is:

<img src="./images/Query 2.png" width="400" height="300">

### Query 2

Now, how about the top 5 interactive users of the Streaming system?
Specifically, the interactivity of a user is determined by how
many times he visits the site and listens to something in that
session. The resulted table should include both the user
information and the number of interactions. This can be answered
using the following query:
```
SELECT user_id, first_name, last_name, gender, level, COUNT(*)
FROM songplays NATURAL JOIN users
GROUP BY user_id, first_name, last_name, gender, level
ORDER BY COUNT(*) DESC LIMIT 5
```
Having executed the above query, the result should be something
like this:

<img src="./images/Query 1.png" width="500" height="300">

## License
Distributed under the MIT License. [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)